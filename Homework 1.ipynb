{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../ADA2017-Tutorials/02 - Intro to Pandas/Data'\n",
    "EBOLA_FOLDER = os.path.join(DATA_FOLDER, 'ebola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Task 1\n",
    "\n",
    "#### Assumptions\n",
    "- The dicionary `schema_and_assumptions` has for each country:\n",
    " 1. `col_date` is the name of the column containing the date\n",
    " 2. `col_desc` the name of the columns containing the description\n",
    " 3. `desc_new_cases` contains the strings that will be considered as new_cases\n",
    " 4. `desc_death` contains the strings that will be considered as death\n",
    " \n",
    " This dicionary will be used to filter each file in the function `get_new_and_death`\n",
    "\n",
    "\n",
    "- The date will be extracted from the file's name because they don't have a standard inside the files, so it would be painful and risky and map all formats\n",
    "\n",
    "- Even for the same country, there are different columns for the files, so the first step is look for all columns available in all files and then for each file it will be selected just the columns that is available (done in function `read_csv_group_concat`)\n",
    "\n",
    "- The function `standardize_date_in_file_name` will standardize the dates\n",
    "\n",
    "- Some columns are not read as numeric type for `sl_data` files, this causes high numbers that looks like outliers but it is just an error when the files are loaded\n",
    "\n",
    "- There missing observations for some dates and countries (specially for Guinea). The missing will not be considered or treated, it means, the mean will be calculated without those observations. This is better than fill with 0's or doing some kind of educated guess.\n",
    "\n",
    "#### Conclusions and Final Comments\n",
    "- Accuracy for `guinea_data` is lower compared with other groups because it just have 22 observed days which is 5x less than the other groups\n",
    "\n",
    "\n",
    "- For `guinea_data` and `sl_data` the number of deaths is much higher than `new_cases` and this might be because:\n",
    "  1. We are not filtering the rights descriptions in the `schema_and_assumptions` for `desc_new_cases` and `desc_death`. This can be fixed with a proper metadada, which is hard to have (rs).\n",
    "  2. It is easier collect deaths than new_cases, so most of the registred cases might be not recorded\n",
    "  3. The period of time that we have observations is biased, might be the end of an outbreak so the number of deaths will be higher than the number os new cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_and_assumptions = {'liberia_data': {'col_date': 'Date',\n",
    "                                           'col_desc': 'Variable',\n",
    "                                           'desc_new_cases': ['New case/s (confirmed)',\n",
    "                                                              'New Case/s (Probable)',\n",
    "                                                              'New Case/s (Suspected)'],\n",
    "                                           'desc_death': ['Total death/s in confirmed cases',\n",
    "                                                          'Total death/s in confirmed cases',\n",
    "                                                          'Total death/s in suspected cases']\n",
    "                                           },\n",
    "\n",
    "                          'sl_data': {'col_date': 'date',\n",
    "                                      'col_desc': 'variable',\n",
    "                                      'desc_new_cases': ['new_confirmed',\n",
    "                                                         'new_probable',\n",
    "                                                         'new_suspected'],\n",
    "                                      'desc_death': ['death_confirmed',\n",
    "                                                     'death_probable',\n",
    "                                                     'death_suspected']\n",
    "                                      },\n",
    "\n",
    "                          'guinea_data': {'col_date': 'Date',\n",
    "                                          'col_desc': 'Description',\n",
    "                                          'desc_new_cases': ['New cases of confirmed',\n",
    "                                                             'New cases of probables',\n",
    "                                                             'New cases of suspects'],\n",
    "                                          'desc_death': ['Total deaths of confirmed',\n",
    "                                                         'Total deaths of probables',\n",
    "                                                         'Total deaths of suspects']\n",
    "                                          }\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date_in_file_name(messed_date):\n",
    "    '''\n",
    "    Receives something like 2014-12-09-v920 and returns 2014-12-09\n",
    "    Hacked and dirty\n",
    "    '''\n",
    "    return messed_date[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_and_death(df, \n",
    "                      country,\n",
    "                      date, \n",
    "                      col_desc, \n",
    "                      cols_to_sum, \n",
    "                      desc_new_cases, \n",
    "                      desc_death):\n",
    "    '''\n",
    "    1. Drop columns that are not in `{date, col_desc, cols_to_sum}`\n",
    "    2. Drop lines that are not in `{desc_new_cases, desc_death}`\n",
    "    3. Convert `cols_to_sum` in numeric (someone them are not, them it causes fake-outliers)\n",
    "    4. For each line it will sum all the columns in cols_to_sum\n",
    "    5. Aggregate sums in the `{new_cases, death}`\n",
    "    6. Standardize columns names\n",
    "    \n",
    "    :returns a dataframe with 1 row and the following columns `{date, country, type {deaths, new_cases}, total}`\n",
    "    '''\n",
    "    \n",
    "    df = df[[col_desc] + cols_to_sum]\n",
    "    df = df[df[col_desc].apply(lambda x: x in desc_new_cases + desc_death)]\n",
    "    df[cols_to_sum] = df[cols_to_sum].fillna(0).apply(pd.to_numeric)\n",
    "\n",
    "    df['total'] = df[cols_to_sum].sum(axis=1)\n",
    "    df['type'] = df[col_desc].apply(lambda x: 'new_cases' if x in desc_new_cases else 'deaths')\n",
    "    \n",
    "    df['date'] = date\n",
    "    agg_df = (df\n",
    "              .groupby(['date', 'type'])\n",
    "              .sum()[['total']].reset_index())\n",
    "    agg_df['country'] = country\n",
    "    \n",
    "    return agg_df[['date', 'country', 'type', 'total']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_group_concat(root_path, schemas):\n",
    "    '''\n",
    "    1. For each country it scan all columns of all files\n",
    "    2. Colect all possible columns and filter that ones that don't make sense like: Unamed, Totals, `col_date` and `col_desc`\n",
    "    3. For each file, it aggregates using the function `get_new_and_death`\n",
    "    4. Concat all the results\n",
    "    \n",
    "    :returns a dataframe with the following columns `{date, country, type {deaths, new_cases}, total}`\n",
    "    '''\n",
    "    \n",
    "    agg_counts = pd.DataFrame()\n",
    "\n",
    "    for country in schemas:\n",
    "        schema = schemas[country]\n",
    "        \n",
    "        country_path = os.path.join(root_path, country)\n",
    "        all_csvs = filter(lambda x: x.endswith('csv'), os.listdir(country_path))\n",
    "        all_paths = list(map(lambda x: os.path.join(country_path, x), all_csvs))\n",
    "        \n",
    "        scan_all_cols = map(lambda p: list(pd.read_csv(p, nrows=0)), all_paths)\n",
    "        all_possible_cols = list(filter(lambda x: not x.startswith('Unnamed') and \n",
    "                                                  not x.startswith('Totals') and\n",
    "                                                  x!=schema['col_date'] and\n",
    "                                                  x!=schema['col_desc'],\n",
    "                                        list(set(sum(scan_all_cols, [])))))\n",
    "    \n",
    "        for file_path in all_paths:\n",
    "            df = pd.read_csv(file_path)\n",
    "            cols_to_sum = list(set(all_possible_cols).intersection(set(df.columns)))\n",
    "            date = standardize_date_in_file_name(os.path.split(file_path)[-1])\n",
    "            \n",
    "            agg_df = get_new_and_death(df,\n",
    "                                       country,\n",
    "                                       date,\n",
    "                                       schema['col_desc'],\n",
    "                                       cols_to_sum,\n",
    "                                       schema['desc_new_cases'],\n",
    "                                       schema['desc_death'])\n",
    "                \n",
    "            agg_counts = pd.concat([agg_counts, agg_df], axis=0)\n",
    "                \n",
    "    agg_counts.date = pd.to_datetime(agg_counts.date)\n",
    "    \n",
    "    return agg_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_by_date = read_csv_group_concat(EBOLA_FOLDER, schema_and_assumptions)\n",
    "mean_by_year = agg_by_date.groupby(['country', 'type']).agg(['size', 'mean']).unstack(level=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily average for the observations between 2014-06-16 and 2014-12-13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">size</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>deaths</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>new_cases</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>guinea_data</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>570.363636</td>\n",
       "      <td>21.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberia_data</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>716.750000</td>\n",
       "      <td>995.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sl_data</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>1735.864078</td>\n",
       "      <td>107.747573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               size                   mean            \n",
       "type         deaths new_cases       deaths   new_cases\n",
       "country                                               \n",
       "guinea_data      22        22   570.363636   21.454545\n",
       "liberia_data    100       100   716.750000  995.280000\n",
       "sl_data         103       103  1735.864078  107.747573"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Daily average for the observations between {} and {}'.format(min(agg_by_date['date']).date(), max(agg_by_date['date']).date()))\n",
    "mean_by_year['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
